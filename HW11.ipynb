{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "This is a companion notebook for the book [Deep Learning with Python, Second Edition](https://www.manning.com/books/deep-learning-with-python-second-edition?a_aid=keras&a_bid=76564dff). For readability, it only contains runnable code blocks and section titles, and omits everything else in the book: text paragraphs, figures, and pseudocode.\n",
    "\n",
    "**If you want to be able to follow what's going on, I recommend reading the notebook side by side with your copy of the book.**\n",
    "\n",
    "This notebook was generated for TensorFlow 2.6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Deep learning for text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Natural-language processing: The bird's eye view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Preparing text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Text standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Text splitting (tokenization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Vocabulary indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Using the TextVectorization layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "class Vectorizer:\n",
    "    def standardize(self, text):\n",
    "        text = text.lower()\n",
    "        return \"\".join(char for char in text if char not in string.punctuation)\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        text = self.standardize(text)\n",
    "        return text.split()\n",
    "\n",
    "    def make_vocabulary(self, dataset):\n",
    "        self.vocabulary = {\"\": 0, \"[UNK]\": 1}\n",
    "        for text in dataset:\n",
    "            text = self.standardize(text)\n",
    "            tokens = self.tokenize(text)\n",
    "            for token in tokens:\n",
    "                if token not in self.vocabulary:\n",
    "                    self.vocabulary[token] = len(self.vocabulary)\n",
    "        self.inverse_vocabulary = dict(\n",
    "            (v, k) for k, v in self.vocabulary.items())\n",
    "\n",
    "    def encode(self, text):\n",
    "        text = self.standardize(text)\n",
    "        tokens = self.tokenize(text)\n",
    "        return [self.vocabulary.get(token, 1) for token in tokens]\n",
    "\n",
    "    def decode(self, int_sequence):\n",
    "        return \" \".join(\n",
    "            self.inverse_vocabulary.get(i, \"[UNK]\") for i in int_sequence)\n",
    "\n",
    "vectorizer = Vectorizer()\n",
    "dataset = [\n",
    "    \"I write, erase, rewrite\",\n",
    "    \"Erase again, and then\",\n",
    "    \"A poppy blooms.\",\n",
    "]\n",
    "vectorizer.make_vocabulary(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### encode字串"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 5, 7, 1, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "test_sentence = \"I write, rewrite, and still rewrite again\"\n",
    "encoded_sentence = vectorizer.encode(test_sentence)\n",
    "print(encoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i write rewrite and [UNK] rewrite again\n"
     ]
    }
   ],
   "source": [
    "decoded_sentence = vectorizer.decode(encoded_sentence)\n",
    "print(decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "text_vectorization = TextVectorization(\n",
    "    output_mode=\"int\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import tensorflow as tf\n",
    "\n",
    "def custom_standardization_fn(string_tensor):\n",
    "    lowercase_string = tf.strings.lower(string_tensor)\n",
    "    return tf.strings.regex_replace(\n",
    "        lowercase_string, f\"[{re.escape(string.punctuation)}]\", \"\")\n",
    "\n",
    "def custom_split_fn(string_tensor):\n",
    "    return tf.strings.split(string_tensor)\n",
    "\n",
    "text_vectorization = TextVectorization(\n",
    "    output_mode=\"int\",\n",
    "    standardize=custom_standardization_fn,\n",
    "    split=custom_split_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "dataset = [\n",
    "    \"I write, erase, rewrite\",\n",
    "    \"Erase again, and then\",\n",
    "    \"A poppy blooms.\",\n",
    "]\n",
    "text_vectorization.adapt(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Displaying the vocabulary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '[UNK]',\n",
       " 'erase',\n",
       " 'write',\n",
       " 'then',\n",
       " 'rewrite',\n",
       " 'poppy',\n",
       " 'i',\n",
       " 'blooms',\n",
       " 'and',\n",
       " 'again',\n",
       " 'a']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vectorization.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 7  3  5  9  1  5 10], shape=(7,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "vocabulary = text_vectorization.get_vocabulary()\n",
    "test_sentence = \"I write, rewrite, and still rewrite again\"\n",
    "encoded_sentence = text_vectorization(test_sentence)\n",
    "print(encoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i write rewrite and [UNK] rewrite again\n"
     ]
    }
   ],
   "source": [
    "inverse_vocab = dict(enumerate(vocabulary))\n",
    "decoded_sentence = \" \".join(inverse_vocab[int(i)] for i in encoded_sentence)\n",
    "print(decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get_vocabulary()時still未納進去，所以decode後顯示unk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Two approaches for representing groups of words: Sets and sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Preparing the IMDB movie reviews data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 準備IMDB資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0 80.2M    0 32768    0     0  24036      0  0:58:19  0:00:01  0:58:18 24041\n",
      "  0 80.2M    0  240k    0     0   106k      0  0:12:49  0:00:02  0:12:47  106k\n",
      "  0 80.2M    0  624k    0     0   188k      0  0:07:16  0:00:03  0:07:13  188k\n",
      "  1 80.2M    1  880k    0     0   201k      0  0:06:48  0:00:04  0:06:44  201k\n",
      "  1 80.2M    1 1088k    0     0   206k      0  0:06:37  0:00:05  0:06:32  215k\n",
      "  1 80.2M    1 1264k    0     0   199k      0  0:06:51  0:00:06  0:06:45  248k\n",
      "  1 80.2M    1 1392k    0     0   193k      0  0:07:05  0:00:07  0:06:58  232k\n",
      "  1 80.2M    1 1568k    0     0   189k      0  0:07:13  0:00:08  0:07:05  190k\n",
      "  2 80.2M    2 1760k    0     0   188k      0  0:07:15  0:00:09  0:07:06  177k\n",
      "  2 80.2M    2 1952k    0     0   190k      0  0:07:10  0:00:10  0:07:00  174k\n",
      "  2 80.2M    2 2080k    0     0   184k      0  0:07:25  0:00:11  0:07:14  164k\n",
      "  2 80.2M    2 2272k    0     0   183k      0  0:07:26  0:00:12  0:07:14  171k\n",
      "  2 80.2M    2 2432k    0     0   183k      0  0:07:27  0:00:13  0:07:14  174k\n",
      "  3 80.2M    3 2624k    0     0   183k      0  0:07:27  0:00:14  0:07:13  174k\n",
      "  3 80.2M    3 2832k    0     0   184k      0  0:07:25  0:00:15  0:07:10  171k\n",
      "  3 80.2M    3 3056k    0     0   188k      0  0:07:16  0:00:16  0:07:00  196k\n",
      "  4 80.2M    4 3392k    0     0   195k      0  0:06:59  0:00:17  0:06:42  225k\n",
      "  4 80.2M    4 3776k    0     0   205k      0  0:06:39  0:00:18  0:06:21  261k\n",
      "  4 80.2M    4 4064k    0     0   210k      0  0:06:29  0:00:19  0:06:10  290k\n",
      "  5 80.2M    5 4288k    0     0   210k      0  0:06:29  0:00:20  0:06:09  293k\n",
      "  5 80.2M    5 4480k    0     0   211k      0  0:06:28  0:00:21  0:06:07  286k\n",
      "  5 80.2M    5 4720k    0     0   211k      0  0:06:27  0:00:22  0:06:05  267k\n",
      "  6 80.2M    6 4944k    0     0   211k      0  0:06:27  0:00:23  0:06:04  235k\n",
      "  6 80.2M    6 5136k    0     0   211k      0  0:06:27  0:00:24  0:06:03  215k\n",
      "  6 80.2M    6 5280k    0     0   208k      0  0:06:33  0:00:25  0:06:08  199k\n",
      "  6 80.2M    6 5472k    0     0   207k      0  0:06:35  0:00:26  0:06:09  193k\n",
      "  6 80.2M    6 5632k    0     0   206k      0  0:06:37  0:00:27  0:06:10  183k\n",
      "  7 80.2M    7 5792k    0     0   204k      0  0:06:41  0:00:28  0:06:13  170k\n",
      "  7 80.2M    7 5968k    0     0   203k      0  0:06:44  0:00:29  0:06:15  162k\n",
      "  7 80.2M    7 6128k    0     0   202k      0  0:06:45  0:00:30  0:06:15  170k\n",
      "  7 80.2M    7 6320k    0     0   201k      0  0:06:47  0:00:31  0:06:16  170k\n",
      "  7 80.2M    7 6528k    0     0   202k      0  0:06:45  0:00:32  0:06:13  180k\n",
      "  8 80.2M    8 6768k    0     0   203k      0  0:06:43  0:00:33  0:06:10  196k\n",
      "  8 80.2M    8 7136k    0     0   207k      0  0:06:35  0:00:34  0:06:01  235k\n",
      "  9 80.2M    9 7456k    0     0   211k      0  0:06:27  0:00:35  0:05:52  267k\n",
      "  9 80.2M    9 7840k    0     0   216k      0  0:06:20  0:00:36  0:05:44  306k\n",
      "  9 80.2M    9 8160k    0     0   218k      0  0:06:15  0:00:37  0:05:38  318k\n",
      " 10 80.2M   10 8432k    0     0   220k      0  0:06:12  0:00:38  0:05:34  335k\n",
      " 10 80.2M   10 8784k    0     0   223k      0  0:06:07  0:00:39  0:05:28  331k\n",
      " 11 80.2M   11 9168k    0     0   227k      0  0:06:00  0:00:40  0:05:20  341k\n",
      " 11 80.2M   11 9488k    0     0   230k      0  0:05:57  0:00:41  0:05:16  331k\n",
      " 12 80.2M   12 9872k    0     0   233k      0  0:05:52  0:00:42  0:05:10  344k\n",
      " 12 80.2M   12 10.0M    0     0   237k      0  0:05:46  0:00:43  0:05:03  363k\n",
      " 12 80.2M   12 10.3M    0     0   238k      0  0:05:43  0:00:44  0:04:59  360k\n",
      " 13 80.2M   13 10.7M    0     0   241k      0  0:05:39  0:00:45  0:04:54  352k\n",
      " 13 80.2M   13 11.0M    0     0   244k      0  0:05:36  0:00:46  0:04:50  360k\n",
      " 14 80.2M   14 11.3M    0     0   246k      0  0:05:32  0:00:47  0:04:45  360k\n",
      " 14 80.2M   14 11.7M    0     0   249k      0  0:05:28  0:00:48  0:04:40  360k\n",
      " 15 80.2M   15 12.0M    0     0   251k      0  0:05:26  0:00:49  0:04:37  360k\n",
      " 15 80.2M   15 12.4M    0     0   253k      0  0:05:23  0:00:50  0:04:33  360k\n",
      " 15 80.2M   15 12.8M    0     0   256k      0  0:05:20  0:00:51  0:04:29  367k\n",
      " 16 80.2M   16 13.1M    0     0   257k      0  0:05:18  0:00:52  0:04:26  360k\n",
      " 16 80.2M   16 13.5M    0     0   259k      0  0:05:16  0:00:53  0:04:23  351k\n",
      " 17 80.2M   17 13.8M    0     0   261k      0  0:05:13  0:00:54  0:04:19  367k\n",
      " 17 80.2M   17 14.1M    0     0   262k      0  0:05:12  0:00:55  0:04:17  357k\n",
      " 18 80.2M   18 14.5M    0     0   264k      0  0:05:10  0:00:56  0:04:14  351k\n",
      " 18 80.2M   18 14.9M    0     0   267k      0  0:05:07  0:00:57  0:04:10  370k\n",
      " 19 80.2M   19 15.2M    0     0   268k      0  0:05:06  0:00:58  0:04:08  357k\n",
      " 19 80.2M   19 15.6M    0     0   269k      0  0:05:04  0:00:59  0:04:05  351k\n",
      " 19 80.2M   19 15.9M    0     0   270k      0  0:05:03  0:01:00  0:04:03  354k\n",
      " 20 80.2M   20 16.1M    0     0   270k      0  0:05:04  0:01:01  0:04:03  331k\n",
      " 20 80.2M   20 16.4M    0     0   270k      0  0:05:04  0:01:02  0:04:02  301k\n",
      " 20 80.2M   20 16.6M    0     0   270k      0  0:05:04  0:01:03  0:04:01  295k\n",
      " 21 80.2M   21 17.0M    0     0   270k      0  0:05:03  0:01:04  0:03:59  286k\n",
      " 21 80.2M   21 17.3M    0     0   271k      0  0:05:02  0:01:05  0:03:57  289k\n",
      " 21 80.2M   21 17.6M    0     0   272k      0  0:05:01  0:01:06  0:03:55  303k\n",
      " 22 80.2M   22 18.0M    0     0   273k      0  0:05:00  0:01:07  0:03:53  319k\n",
      " 22 80.2M   22 18.2M    0     0   273k      0  0:04:59  0:01:08  0:03:51  321k\n",
      " 23 80.2M   23 18.5M    0     0   274k      0  0:04:59  0:01:09  0:03:50  318k\n",
      " 23 80.2M   23 18.8M    0     0   274k      0  0:04:58  0:01:10  0:03:48  313k\n",
      " 23 80.2M   23 19.1M    0     0   274k      0  0:04:58  0:01:11  0:03:47  305k\n",
      " 24 80.2M   24 19.4M    0     0   276k      0  0:04:57  0:01:12  0:03:45  305k\n",
      " 24 80.2M   24 19.8M    0     0   277k      0  0:04:55  0:01:13  0:03:42  326k\n",
      " 25 80.2M   25 20.0M    0     0   277k      0  0:04:56  0:01:14  0:03:42  315k\n",
      " 25 80.2M   25 20.3M    0     0   277k      0  0:04:56  0:01:15  0:03:41  311k\n",
      " 25 80.2M   25 20.7M    0     0   278k      0  0:04:55  0:01:16  0:03:39  323k\n",
      " 26 80.2M   26 21.0M    0     0   279k      0  0:04:54  0:01:17  0:03:37  325k\n",
      " 26 80.2M   26 21.4M    0     0   280k      0  0:04:53  0:01:18  0:03:35  319k\n",
      " 27 80.2M   27 21.7M    0     0   281k      0  0:04:51  0:01:19  0:03:32  347k\n",
      " 27 80.2M   27 22.1M    0     0   282k      0  0:04:51  0:01:20  0:03:31  357k\n",
      " 28 80.2M   28 22.5M    0     0   283k      0  0:04:49  0:01:21  0:03:28  360k\n",
      " 28 80.2M   28 22.8M    0     0   284k      0  0:04:49  0:01:22  0:03:27  360k\n",
      " 28 80.2M   28 23.1M    0     0   285k      0  0:04:48  0:01:23  0:03:25  360k\n",
      " 29 80.2M   29 23.5M    0     0   286k      0  0:04:47  0:01:24  0:03:23  355k\n",
      " 29 80.2M   29 23.8M    0     0   286k      0  0:04:46  0:01:25  0:03:21  360k\n",
      " 30 80.2M   30 24.2M    0     0   287k      0  0:04:46  0:01:26  0:03:20  350k\n",
      " 30 80.2M   30 24.5M    0     0   287k      0  0:04:46  0:01:27  0:03:19  335k\n",
      " 30 80.2M   30 24.7M    0     0   287k      0  0:04:45  0:01:28  0:03:17  331k\n",
      " 31 80.2M   31 25.1M    0     0   288k      0  0:04:44  0:01:29  0:03:15  331k\n",
      " 31 80.2M   31 25.4M    0     0   288k      0  0:04:44  0:01:30  0:03:14  317k\n",
      " 32 80.2M   32 25.7M    0     0   288k      0  0:04:44  0:01:31  0:03:13  312k\n",
      " 32 80.2M   32 26.0M    0     0   289k      0  0:04:44  0:01:32  0:03:12  322k\n",
      " 32 80.2M   32 26.3M    0     0   289k      0  0:04:43  0:01:33  0:03:10  325k\n",
      " 33 80.2M   33 26.7M    0     0   290k      0  0:04:42  0:01:34  0:03:08  325k\n",
      " 33 80.2M   33 27.1M    0     0   291k      0  0:04:41  0:01:35  0:03:06  341k\n",
      " 34 80.2M   34 27.3M    0     0   291k      0  0:04:42  0:01:36  0:03:06  338k\n",
      " 34 80.2M   34 27.6M    0     0   291k      0  0:04:42  0:01:37  0:03:05  331k\n",
      " 34 80.2M   34 28.0M    0     0   291k      0  0:04:41  0:01:38  0:03:03  329k\n",
      " 35 80.2M   35 28.3M    0     0   292k      0  0:04:40  0:01:39  0:03:01  328k\n",
      " 35 80.2M   35 28.7M    0     0   293k      0  0:04:40  0:01:40  0:03:00  328k\n",
      " 36 80.2M   36 29.0M    0     0   293k      0  0:04:39  0:01:41  0:02:58  345k\n",
      " 36 80.2M   36 29.4M    0     0   294k      0  0:04:38  0:01:42  0:02:56  357k\n",
      " 37 80.2M   37 29.7M    0     0   295k      0  0:04:38  0:01:43  0:02:55  360k\n",
      " 37 80.2M   37 30.0M    0     0   295k      0  0:04:37  0:01:44  0:02:53  360k\n",
      " 37 80.2M   37 30.4M    0     0   296k      0  0:04:37  0:01:45  0:02:52  360k\n",
      " 38 80.2M   38 30.8M    0     0   297k      0  0:04:36  0:01:46  0:02:50  360k\n",
      " 38 80.2M   38 31.1M    0     0   297k      0  0:04:36  0:01:47  0:02:49  360k\n",
      " 39 80.2M   39 31.5M    0     0   298k      0  0:04:35  0:01:48  0:02:47  360k\n",
      " 39 80.2M   39 31.9M    0     0   298k      0  0:04:34  0:01:49  0:02:45  361k\n",
      " 40 80.2M   40 32.2M    0     0   299k      0  0:04:34  0:01:50  0:02:44  360k\n",
      " 40 80.2M   40 32.5M    0     0   299k      0  0:04:33  0:01:51  0:02:42  360k\n",
      " 41 80.2M   41 32.9M    0     0   300k      0  0:04:33  0:01:52  0:02:41  351k\n",
      " 41 80.2M   41 33.2M    0     0   300k      0  0:04:33  0:01:53  0:02:40  350k\n",
      " 41 80.2M   41 33.6M    0     0   301k      0  0:04:32  0:01:54  0:02:38  350k\n",
      " 42 80.2M   42 33.9M    0     0   301k      0  0:04:32  0:01:55  0:02:37  353k\n",
      " 42 80.2M   42 34.2M    0     0   302k      0  0:04:31  0:01:56  0:02:35  350k\n",
      " 43 80.2M   43 34.6M    0     0   302k      0  0:04:31  0:01:57  0:02:34  361k\n",
      " 43 80.2M   43 34.9M    0     0   303k      0  0:04:31  0:01:58  0:02:33  361k\n",
      " 44 80.2M   44 35.3M    0     0   303k      0  0:04:30  0:01:59  0:02:31  361k\n",
      " 44 80.2M   44 35.7M    0     0   304k      0  0:04:30  0:02:00  0:02:30  358k\n",
      " 44 80.2M   44 36.0M    0     0   304k      0  0:04:29  0:02:01  0:02:28  361k\n",
      " 45 80.2M   45 36.4M    0     0   304k      0  0:04:29  0:02:02  0:02:27  360k\n",
      " 45 80.2M   45 36.7M    0     0   305k      0  0:04:29  0:02:03  0:02:26  357k\n",
      " 46 80.2M   46 37.0M    0     0   305k      0  0:04:28  0:02:04  0:02:24  357k\n",
      " 46 80.2M   46 37.4M    0     0   306k      0  0:04:28  0:02:05  0:02:23  357k\n",
      " 47 80.2M   47 37.8M    0     0   306k      0  0:04:27  0:02:06  0:02:21  366k\n",
      " 47 80.2M   47 38.1M    0     0   307k      0  0:04:27  0:02:07  0:02:20  357k\n",
      " 48 80.2M   48 38.5M    0     0   307k      0  0:04:27  0:02:08  0:02:19  360k\n",
      " 48 80.2M   48 38.8M    0     0   307k      0  0:04:26  0:02:09  0:02:17  361k\n",
      " 48 80.2M   48 39.2M    0     0   308k      0  0:04:26  0:02:10  0:02:16  361k\n",
      " 49 80.2M   49 39.5M    0     0   308k      0  0:04:26  0:02:11  0:02:15  352k\n",
      " 49 80.2M   49 39.9M    0     0   309k      0  0:04:25  0:02:12  0:02:13  361k\n",
      " 50 80.2M   50 40.2M    0     0   309k      0  0:04:25  0:02:13  0:02:12  361k\n",
      " 50 80.2M   50 40.6M    0     0   309k      0  0:04:25  0:02:14  0:02:11  361k\n",
      " 51 80.2M   51 40.9M    0     0   310k      0  0:04:24  0:02:15  0:02:09  361k\n",
      " 51 80.2M   51 41.3M    0     0   310k      0  0:04:24  0:02:16  0:02:08  361k\n",
      " 52 80.2M   52 41.7M    0     0   310k      0  0:04:24  0:02:17  0:02:07  358k\n",
      " 52 80.2M   52 41.9M    0     0   310k      0  0:04:24  0:02:18  0:02:06  335k\n",
      " 52 80.2M   52 42.1M    0     0   309k      0  0:04:25  0:02:19  0:02:06  306k\n",
      " 52 80.2M   52 42.4M    0     0   309k      0  0:04:25  0:02:20  0:02:05  286k\n",
      " 53 80.2M   53 42.6M    0     0   308k      0  0:04:25  0:02:21  0:02:04  264k\n",
      " 53 80.2M   53 42.8M    0     0   308k      0  0:04:26  0:02:22  0:02:04  240k\n",
      " 53 80.2M   53 43.1M    0     0   308k      0  0:04:26  0:02:23  0:02:03  247k\n",
      " 54 80.2M   54 43.4M    0     0   308k      0  0:04:26  0:02:24  0:02:02  264k\n",
      " 54 80.2M   54 43.7M    0     0   308k      0  0:04:26  0:02:25  0:02:01  286k\n",
      " 54 80.2M   54 44.1M    0     0   308k      0  0:04:25  0:02:26  0:01:59  305k\n",
      " 55 80.2M   55 44.4M    0     0   309k      0  0:04:25  0:02:27  0:01:58  328k\n",
      " 55 80.2M   55 44.8M    0     0   309k      0  0:04:25  0:02:28  0:01:57  345k\n",
      " 56 80.2M   56 45.1M    0     0   309k      0  0:04:25  0:02:29  0:01:56  357k\n",
      " 56 80.2M   56 45.4M    0     0   309k      0  0:04:25  0:02:30  0:01:55  338k\n",
      " 57 80.2M   57 45.7M    0     0   309k      0  0:04:25  0:02:31  0:01:54  332k\n",
      " 57 80.2M   57 45.9M    0     0   309k      0  0:04:25  0:02:32  0:01:53  302k\n",
      " 57 80.2M   57 46.2M    0     0   308k      0  0:04:26  0:02:33  0:01:53  286k\n",
      " 57 80.2M   57 46.4M    0     0   308k      0  0:04:26  0:02:34  0:01:52  270k\n",
      " 58 80.2M   58 46.7M    0     0   308k      0  0:04:26  0:02:35  0:01:51  270k\n",
      " 58 80.2M   58 46.9M    0     0   307k      0  0:04:27  0:02:36  0:01:51  244k\n",
      " 58 80.2M   58 47.1M    0     0   307k      0  0:04:27  0:02:37  0:01:50  254k\n",
      " 59 80.2M   59 47.4M    0     0   307k      0  0:04:27  0:02:38  0:01:49  257k\n",
      " 59 80.2M   59 47.7M    0     0   307k      0  0:04:27  0:02:39  0:01:48  261k\n",
      " 59 80.2M   59 48.0M    0     0   306k      0  0:04:27  0:02:40  0:01:47  260k\n",
      " 60 80.2M   60 48.3M    0     0   306k      0  0:04:27  0:02:41  0:01:46  283k\n",
      " 60 80.2M   60 48.7M    0     0   307k      0  0:04:27  0:02:42  0:01:45  301k\n",
      " 61 80.2M   61 49.0M    0     0   307k      0  0:04:27  0:02:43  0:01:44  315k\n",
      " 61 80.2M   61 49.3M    0     0   307k      0  0:04:26  0:02:44  0:01:42  332k\n",
      " 61 80.2M   61 49.7M    0     0   308k      0  0:04:26  0:02:45  0:01:41  348k\n",
      " 62 80.2M   62 50.0M    0     0   308k      0  0:04:26  0:02:46  0:01:40  357k\n",
      " 62 80.2M   62 50.4M    0     0   308k      0  0:04:26  0:02:47  0:01:39  361k\n",
      " 63 80.2M   63 50.7M    0     0   308k      0  0:04:26  0:02:48  0:01:38  347k\n",
      " 63 80.2M   63 51.0M    0     0   308k      0  0:04:26  0:02:49  0:01:37  337k\n",
      " 64 80.2M   64 51.4M    0     0   308k      0  0:04:25  0:02:50  0:01:35  338k\n",
      " 64 80.2M   64 51.7M    0     0   309k      0  0:04:25  0:02:51  0:01:34  337k\n",
      " 64 80.2M   64 52.0M    0     0   309k      0  0:04:25  0:02:52  0:01:33  337k\n",
      " 65 80.2M   65 52.4M    0     0   309k      0  0:04:25  0:02:53  0:01:32  351k\n",
      " 65 80.2M   65 52.7M    0     0   310k      0  0:04:24  0:02:54  0:01:30  360k\n",
      " 66 80.2M   66 53.1M    0     0   310k      0  0:04:24  0:02:55  0:01:29  360k\n",
      " 66 80.2M   66 53.4M    0     0   310k      0  0:04:24  0:02:56  0:01:28  360k\n",
      " 67 80.2M   67 53.8M    0     0   311k      0  0:04:24  0:02:57  0:01:27  360k\n",
      " 67 80.2M   67 54.2M    0     0   311k      0  0:04:23  0:02:58  0:01:25  361k\n",
      " 67 80.2M   67 54.5M    0     0   311k      0  0:04:23  0:02:59  0:01:24  361k\n",
      " 68 80.2M   68 54.9M    0     0   311k      0  0:04:23  0:03:00  0:01:23  360k\n",
      " 68 80.2M   68 55.2M    0     0   312k      0  0:04:23  0:03:01  0:01:22  361k\n",
      " 69 80.2M   69 55.5M    0     0   312k      0  0:04:22  0:03:02  0:01:20  360k\n",
      " 69 80.2M   69 55.9M    0     0   312k      0  0:04:22  0:03:03  0:01:19  360k\n",
      " 70 80.2M   70 56.3M    0     0   312k      0  0:04:22  0:03:04  0:01:18  361k\n",
      " 70 80.2M   70 56.6M    0     0   313k      0  0:04:22  0:03:05  0:01:17  357k\n",
      " 71 80.2M   71 57.0M    0     0   313k      0  0:04:22  0:03:06  0:01:16  357k\n",
      " 71 80.2M   71 57.3M    0     0   313k      0  0:04:21  0:03:07  0:01:14  366k\n",
      " 71 80.2M   71 57.7M    0     0   313k      0  0:04:21  0:03:08  0:01:13  357k\n",
      " 72 80.2M   72 58.0M    0     0   314k      0  0:04:21  0:03:09  0:01:12  357k\n",
      " 72 80.2M   72 58.3M    0     0   314k      0  0:04:21  0:03:10  0:01:11  361k\n",
      " 73 80.2M   73 58.7M    0     0   314k      0  0:04:21  0:03:11  0:01:10  361k\n",
      " 73 80.2M   73 59.0M    0     0   314k      0  0:04:21  0:03:12  0:01:09  341k\n",
      " 73 80.2M   73 59.3M    0     0   314k      0  0:04:21  0:03:13  0:01:08  333k\n",
      " 74 80.2M   74 59.6M    0     0   314k      0  0:04:21  0:03:14  0:01:07  314k\n",
      " 74 80.2M   74 59.9M    0     0   313k      0  0:04:21  0:03:15  0:01:06  300k\n",
      " 74 80.2M   74 60.1M    0     0   313k      0  0:04:21  0:03:16  0:01:05  285k\n",
      " 75 80.2M   75 60.4M    0     0   313k      0  0:04:21  0:03:17  0:01:04  280k\n",
      " 75 80.2M   75 60.7M    0     0   313k      0  0:04:21  0:03:18  0:01:03  286k\n",
      " 76 80.2M   76 61.0M    0     0   313k      0  0:04:21  0:03:19  0:01:02  302k\n",
      " 76 80.2M   76 61.4M    0     0   314k      0  0:04:21  0:03:20  0:01:01  318k\n",
      " 76 80.2M   76 61.7M    0     0   314k      0  0:04:21  0:03:21  0:01:00  331k\n",
      " 77 80.2M   77 62.0M    0     0   314k      0  0:04:21  0:03:22  0:00:59  331k\n",
      " 77 80.2M   77 62.3M    0     0   313k      0  0:04:21  0:03:23  0:00:58  322k\n",
      " 77 80.2M   77 62.5M    0     0   313k      0  0:04:22  0:03:24  0:00:58  291k\n",
      " 78 80.2M   78 62.6M    0     0   312k      0  0:04:22  0:03:25  0:00:57  249k\n",
      " 78 80.2M   78 62.7M    0     0   311k      0  0:04:23  0:03:26  0:00:57  194k\n",
      " 78 80.2M   78 62.7M    0     0   310k      0  0:04:24  0:03:27  0:00:57  149k\n",
      " 78 80.2M   78 62.8M    0     0   308k      0  0:04:25  0:03:28  0:00:57  102k\n",
      " 78 80.2M   78 62.8M    0     0   307k      0  0:04:27  0:03:29  0:00:58 78189\n",
      " 78 80.2M   78 62.9M    0     0   306k      0  0:04:28  0:03:30  0:00:58 58829\n",
      " 78 80.2M   78 63.0M    0     0   305k      0  0:04:28  0:03:31  0:00:57 65366\n",
      " 78 80.2M   78 63.1M    0     0   304k      0  0:04:29  0:03:32  0:00:57 65470\n",
      " 78 80.2M   78 63.1M    0     0   303k      0  0:04:30  0:03:33  0:00:57 75441\n",
      " 78 80.2M   78 63.2M    0     0   302k      0  0:04:31  0:03:34  0:00:57 82232\n",
      " 78 80.2M   78 63.3M    0     0   301k      0  0:04:32  0:03:35  0:00:57 85401\n",
      " 79 80.2M   79 63.4M    0     0   300k      0  0:04:33  0:03:36  0:00:57 91934\n",
      " 79 80.2M   79 63.6M    0     0   300k      0  0:04:33  0:03:37  0:00:56  118k\n",
      " 79 80.2M   79 63.9M    0     0   299k      0  0:04:33  0:03:38  0:00:55  148k\n",
      " 79 80.2M   79 64.1M    0     0   299k      0  0:04:34  0:03:39  0:00:55  179k\n",
      " 80 80.2M   80 64.4M    0     0   299k      0  0:04:34  0:03:40  0:00:54  220k\n",
      " 80 80.2M   80 64.6M    0     0   299k      0  0:04:34  0:03:41  0:00:53  234k\n",
      " 80 80.2M   80 64.7M    0     0   298k      0  0:04:35  0:03:42  0:00:53  217k\n",
      " 80 80.2M   80 64.8M    0     0   297k      0  0:04:36  0:03:43  0:00:53  192k\n",
      " 80 80.2M   80 64.9M    0     0   296k      0  0:04:37  0:03:44  0:00:53  166k\n",
      " 81 80.2M   81 65.0M    0     0   295k      0  0:04:37  0:03:45  0:00:52  133k\n",
      " 81 80.2M   81 65.1M    0     0   295k      0  0:04:38  0:03:46  0:00:52  115k\n",
      " 81 80.2M   81 65.3M    0     0   294k      0  0:04:39  0:03:47  0:00:52  122k\n",
      " 81 80.2M   81 65.5M    0     0   294k      0  0:04:39  0:03:48  0:00:51  141k\n",
      " 82 80.2M   82 65.8M    0     0   294k      0  0:04:39  0:03:49  0:00:50  193k\n",
      " 82 80.2M   82 66.2M    0     0   294k      0  0:04:38  0:03:50  0:00:48  245k\n",
      " 82 80.2M   82 66.5M    0     0   294k      0  0:04:38  0:03:51  0:00:47  276k\n",
      " 83 80.2M   83 66.8M    0     0   294k      0  0:04:38  0:03:52  0:00:46  302k\n",
      " 83 80.2M   83 67.1M    0     0   294k      0  0:04:38  0:03:53  0:00:45  327k\n",
      " 84 80.2M   84 67.5M    0     0   295k      0  0:04:38  0:03:54  0:00:44  330k\n",
      " 84 80.2M   84 67.8M    0     0   295k      0  0:04:38  0:03:55  0:00:43  326k\n",
      " 85 80.2M   85 68.2M    0     0   295k      0  0:04:37  0:03:56  0:00:41  345k\n",
      " 85 80.2M   85 68.5M    0     0   296k      0  0:04:37  0:03:57  0:00:40  367k\n",
      " 85 80.2M   85 68.9M    0     0   296k      0  0:04:37  0:03:58  0:00:39  361k\n",
      " 86 80.2M   86 69.2M    0     0   296k      0  0:04:37  0:03:59  0:00:38  361k\n",
      " 86 80.2M   86 69.6M    0     0   296k      0  0:04:36  0:04:00  0:00:36  361k\n",
      " 87 80.2M   87 69.9M    0     0   297k      0  0:04:36  0:04:01  0:00:35  361k\n",
      " 87 80.2M   87 70.2M    0     0   296k      0  0:04:36  0:04:02  0:00:34  330k\n",
      " 87 80.2M   87 70.5M    0     0   296k      0  0:04:36  0:04:03  0:00:33  332k\n",
      " 88 80.2M   88 70.9M    0     0   297k      0  0:04:36  0:04:04  0:00:32  332k\n",
      " 88 80.2M   88 71.2M    0     0   297k      0  0:04:36  0:04:05  0:00:31  333k\n",
      " 89 80.2M   89 71.5M    0     0   297k      0  0:04:35  0:04:06  0:00:29  332k\n",
      " 89 80.2M   89 71.9M    0     0   298k      0  0:04:35  0:04:07  0:00:28  354k\n",
      " 90 80.2M   90 72.3M    0     0   298k      0  0:04:35  0:04:08  0:00:27  361k\n",
      " 90 80.2M   90 72.6M    0     0   298k      0  0:04:35  0:04:09  0:00:26  361k\n",
      " 91 80.2M   91 73.0M    0     0   298k      0  0:04:34  0:04:10  0:00:24  361k\n",
      " 91 80.2M   91 73.3M    0     0   299k      0  0:04:34  0:04:11  0:00:23  366k\n",
      " 91 80.2M   91 73.7M    0     0   299k      0  0:04:34  0:04:12  0:00:22  361k\n",
      " 92 80.2M   92 74.0M    0     0   299k      0  0:04:34  0:04:13  0:00:21  361k\n",
      " 92 80.2M   92 74.4M    0     0   299k      0  0:04:34  0:04:14  0:00:20  366k\n",
      " 93 80.2M   93 74.6M    0     0   299k      0  0:04:34  0:04:15  0:00:19  337k\n",
      " 93 80.2M   93 74.9M    0     0   299k      0  0:04:34  0:04:16  0:00:18  320k\n",
      " 93 80.2M   93 75.2M    0     0   299k      0  0:04:34  0:04:17  0:00:17  311k\n",
      " 94 80.2M   94 75.5M    0     0   299k      0  0:04:34  0:04:18  0:00:16  298k\n",
      " 94 80.2M   94 75.8M    0     0   299k      0  0:04:34  0:04:19  0:00:15  286k\n",
      " 94 80.2M   94 76.1M    0     0   299k      0  0:04:34  0:04:20  0:00:14  312k\n",
      " 95 80.2M   95 76.5M    0     0   300k      0  0:04:33  0:04:21  0:00:12  325k\n",
      " 95 80.2M   95 76.8M    0     0   300k      0  0:04:33  0:04:22  0:00:11  335k\n",
      " 96 80.2M   96 77.2M    0     0   300k      0  0:04:33  0:04:23  0:00:10  348k\n",
      " 96 80.2M   96 77.6M    0     0   300k      0  0:04:33  0:04:24  0:00:09  357k\n",
      " 97 80.2M   97 77.9M    0     0   300k      0  0:04:33  0:04:25  0:00:08  357k\n",
      " 97 80.2M   97 78.2M    0     0   301k      0  0:04:32  0:04:26  0:00:06  357k\n",
      " 98 80.2M   98 78.6M    0     0   301k      0  0:04:32  0:04:27  0:00:05  361k\n",
      " 98 80.2M   98 78.9M    0     0   301k      0  0:04:32  0:04:28  0:00:04  344k\n",
      " 98 80.2M   98 79.2M    0     0   301k      0  0:04:32  0:04:29  0:00:03  331k\n",
      " 99 80.2M   99 79.5M    0     0   301k      0  0:04:32  0:04:30  0:00:02  332k\n",
      " 99 80.2M   99 79.9M    0     0   301k      0  0:04:32  0:04:31  0:00:01  331k\n",
      "100 80.2M  100 80.2M    0     0   301k      0  0:04:32  0:04:32 --:--:--  331k\n"
     ]
    }
   ],
   "source": [
    "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "!tar -xf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!rm -r aclImdb/train/unsup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I first saw this back in the early 90s on UK TV, i did like it then but i missed the chance to tape it, many years passed but the film always stuck with me and i lost hope of seeing it TV again, the main thing that stuck with me was the end, the hole castle part really touched me, its easy to watch, has a great story, great music, the list goes on and on, its OK me saying how good it is but everyone will take there own best bits away with them once they have seen it, yes the animation is top notch and beautiful to watch, it does show its age in a very few parts but that has now become part of it beauty, i am so glad it has came out on DVD as it is one of my top 10 films of all time. Buy it or rent it just see it, best viewing is at night alone with drink and food in reach so you don't have to stop the film.<br /><br />Enjoy\n"
     ]
    }
   ],
   "source": [
    "!type 4077_10.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pathlib, shutil, random\n",
    "\n",
    "base_dir = pathlib.Path(\"aclImdb\")\n",
    "val_dir = base_dir / \"val\"\n",
    "train_dir = base_dir / \"train\"\n",
    "for category in (\"neg\", \"pos\"):\n",
    "    os.makedirs(val_dir / category)\n",
    "    files = os.listdir(train_dir / category)\n",
    "    random.Random(1337).shuffle(files)\n",
    "    num_val_samples = int(0.2 * len(files))\n",
    "    val_files = files[-num_val_samples:]\n",
    "    for fname in val_files:\n",
    "        shutil.move(train_dir / category / fname,\n",
    "                    val_dir / category / fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 files belonging to 2 classes.\n",
      "Found 5000 files belonging to 2 classes.\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "batch_size = 32\n",
    "\n",
    "train_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/train\", batch_size=batch_size\n",
    ")\n",
    "val_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/val\", batch_size=batch_size\n",
    ")\n",
    "test_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/test\", batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Displaying the shapes and dtypes of the first batch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs.shape: (32,)\n",
      "inputs.dtype: <dtype: 'string'>\n",
      "targets.shape: (32,)\n",
      "targets.dtype: <dtype: 'int32'>\n",
      "inputs[0]: tf.Tensor(b'I was living Rawlins when this movie was made and I got lucky enough to be able to work on it. Both as an extra and with Eddie Surkin on special effects. It was fun to see all the behind the scene workings, from the Barbedwire coming alive to the Electric chair up through the wardens office floor. Also it was a lot of fun getting to meet all the actors, from Viggo to Tiny. Also the gate that was cut into the prison wall for the movie was and still is called \"Disney Gate\" by locals. If anybody is interested and is ever in Rawlins, most of the movies sets are still in place and can be seen during the self guided tour. It was a lot of fun working for and with R. Harlin and wished I had a chance to do it again.', shape=(), dtype=string)\n",
      "targets[0]: tf.Tensor(1, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_ds:\n",
    "    print(\"inputs.shape:\", inputs.shape)\n",
    "    print(\"inputs.dtype:\", inputs.dtype)\n",
    "    print(\"targets.shape:\", targets.shape)\n",
    "    print(\"targets.dtype:\", targets.dtype)\n",
    "    print(\"inputs[0]:\", inputs[0])\n",
    "    print(\"targets[0]:\", targets[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Processing words as a set: The bag-of-words approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "#### Single words (unigrams) with binary encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Preprocessing our datasets with a `TextVectorization` layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "text_vectorization = TextVectorization(\n",
    "    max_tokens=20000,\n",
    "    output_mode=\"multi_hot\",\n",
    ")\n",
    "text_only_train_ds = train_ds.map(lambda x, y: x)\n",
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "binary_1gram_train_ds = train_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "binary_1gram_val_ds = val_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "binary_1gram_test_ds = test_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Inspecting the output of our binary unigram dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs.shape: (32, 20000)\n",
      "inputs.dtype: <dtype: 'float32'>\n",
      "targets.shape: (32,)\n",
      "targets.dtype: <dtype: 'int32'>\n",
      "inputs[0]: tf.Tensor([0. 1. 1. ... 0. 0. 0.], shape=(20000,), dtype=float32)\n",
      "targets[0]: tf.Tensor(1, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in binary_1gram_train_ds:\n",
    "    print(\"inputs.shape:\", inputs.shape)\n",
    "    print(\"inputs.dtype:\", inputs.dtype)\n",
    "    print(\"targets.shape:\", targets.shape)\n",
    "    print(\"targets.dtype:\", targets.dtype)\n",
    "    print(\"inputs[0]:\", inputs[0])\n",
    "    print(\"targets[0]:\", targets[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Our model-building utility**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def get_model(max_tokens=20000, hidden_dim=16):\n",
    "    inputs = keras.Input(shape=(max_tokens,))\n",
    "    x = layers.Dense(hidden_dim, activation=\"relu\")(inputs)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(optimizer=\"rmsprop\",\n",
    "                  loss=\"binary_crossentropy\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Training and testing the binary unigram model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         [(None, 20000)]           0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                320016    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 320,033\n",
      "Trainable params: 320,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 4s 7ms/step - loss: 0.3965 - accuracy: 0.8328 - val_loss: 0.3027 - val_accuracy: 0.8814\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.2645 - accuracy: 0.9038 - val_loss: 0.3032 - val_accuracy: 0.8858\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.2356 - accuracy: 0.9166 - val_loss: 0.3197 - val_accuracy: 0.8890\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.2169 - accuracy: 0.9271 - val_loss: 0.3425 - val_accuracy: 0.8896\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.2139 - accuracy: 0.9309 - val_loss: 0.3527 - val_accuracy: 0.8882\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.2094 - accuracy: 0.9339 - val_loss: 0.3696 - val_accuracy: 0.8866\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.2022 - accuracy: 0.9352 - val_loss: 0.3802 - val_accuracy: 0.8888\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.2091 - accuracy: 0.9356 - val_loss: 0.3885 - val_accuracy: 0.8848\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.2060 - accuracy: 0.9358 - val_loss: 0.3958 - val_accuracy: 0.8858\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.1982 - accuracy: 0.9377 - val_loss: 0.4051 - val_accuracy: 0.8864\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.2954 - accuracy: 0.8860\n",
      "Test acc: 0.886\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "model.summary()\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"binary_1gram.keras\",\n",
    "                                    save_best_only=True)\n",
    "]\n",
    "model.fit(binary_1gram_train_ds.cache(),\n",
    "          validation_data=binary_1gram_val_ds.cache(),\n",
    "          epochs=10,\n",
    "          callbacks=callbacks)\n",
    "model = keras.models.load_model(\"binary_1gram.keras\")\n",
    "print(f\"Test acc: {model.evaluate(binary_1gram_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "#### Bigrams with binary encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Configuring the `TextVectorization` layer to return bigrams**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "text_vectorization = TextVectorization(\n",
    "    ngrams=2,\n",
    "    max_tokens=20000,\n",
    "    output_mode=\"multi_hot\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Training and testing the binary bigram model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         [(None, 20000)]           0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                320016    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 320,033\n",
      "Trainable params: 320,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 5s 7ms/step - loss: 0.3916 - accuracy: 0.8383 - val_loss: 0.2910 - val_accuracy: 0.8870\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.2678 - accuracy: 0.9104 - val_loss: 0.2942 - val_accuracy: 0.8898\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.2330 - accuracy: 0.9237 - val_loss: 0.3199 - val_accuracy: 0.8896\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.2259 - accuracy: 0.9296 - val_loss: 0.3549 - val_accuracy: 0.8824\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.2201 - accuracy: 0.9327 - val_loss: 0.3547 - val_accuracy: 0.8836\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.2112 - accuracy: 0.9352 - val_loss: 0.3736 - val_accuracy: 0.8818\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.2070 - accuracy: 0.9386 - val_loss: 0.4074 - val_accuracy: 0.8790\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.2083 - accuracy: 0.9372 - val_loss: 0.4233 - val_accuracy: 0.8734\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.2018 - accuracy: 0.9408 - val_loss: 0.4401 - val_accuracy: 0.8688\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.2000 - accuracy: 0.9414 - val_loss: 0.4770 - val_accuracy: 0.8698\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.2800 - accuracy: 0.8943\n",
      "Test acc: 0.894\n"
     ]
    }
   ],
   "source": [
    "text_vectorization.adapt(text_only_train_ds)\n",
    "binary_2gram_train_ds = train_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "binary_2gram_val_ds = val_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "binary_2gram_test_ds = test_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "\n",
    "model = get_model()\n",
    "model.summary()\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"binary_2gram.keras\",\n",
    "                                    save_best_only=True)\n",
    "]\n",
    "model.fit(binary_2gram_train_ds.cache(),\n",
    "          validation_data=binary_2gram_val_ds.cache(),\n",
    "          epochs=10,\n",
    "          callbacks=callbacks)\n",
    "model = keras.models.load_model(\"binary_2gram.keras\")\n",
    "print(f\"Test acc: {model.evaluate(binary_2gram_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "#### Bigrams with TF-IDF encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Configuring the `TextVectorization` layer to return token counts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "text_vectorization = TextVectorization(\n",
    "    ngrams=2,\n",
    "    max_tokens=20000,\n",
    "    output_mode=\"count\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Configuring `TextVectorization` to return TF-IDF-weighted outputs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "text_vectorization = TextVectorization(\n",
    "    ngrams=2,\n",
    "    max_tokens=20000,\n",
    "    output_mode=\"tf_idf\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Training and testing the TF-IDF bigram model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_11 (InputLayer)        [(None, 20000)]           0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                320016    \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 320,033\n",
      "Trainable params: 320,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 4s 7ms/step - loss: 0.5024 - accuracy: 0.7710 - val_loss: 0.3119 - val_accuracy: 0.8892\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.3558 - accuracy: 0.8509 - val_loss: 0.3225 - val_accuracy: 0.8790\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.3191 - accuracy: 0.8609 - val_loss: 0.3510 - val_accuracy: 0.8734\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.2977 - accuracy: 0.8702 - val_loss: 0.3433 - val_accuracy: 0.8716\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.2945 - accuracy: 0.8721 - val_loss: 0.3306 - val_accuracy: 0.8750\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.2823 - accuracy: 0.8766 - val_loss: 0.3586 - val_accuracy: 0.8560\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.2765 - accuracy: 0.8808 - val_loss: 0.3610 - val_accuracy: 0.8576\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.2718 - accuracy: 0.8857 - val_loss: 0.3605 - val_accuracy: 0.8786\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.2688 - accuracy: 0.8848 - val_loss: 0.3753 - val_accuracy: 0.8606\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.2704 - accuracy: 0.8845 - val_loss: 0.3579 - val_accuracy: 0.8542\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.3036 - accuracy: 0.8834\n",
      "Test acc: 0.883\n"
     ]
    }
   ],
   "source": [
    "#text_vectorization.adapt(text_only_train_ds)\n",
    "#tf 2.6無法使用text_vectorization.adapt 會有問題\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tfidf_2gram_train_ds = train_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "tfidf_2gram_val_ds = val_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "tfidf_2gram_test_ds = test_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "\n",
    "model = get_model()\n",
    "model.summary()\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"tfidf_2gram.keras\",\n",
    "                                    save_best_only=True)\n",
    "]\n",
    "model.fit(tfidf_2gram_train_ds.cache(),\n",
    "          validation_data=tfidf_2gram_val_ds.cache(),\n",
    "          epochs=10,\n",
    "          callbacks=callbacks)\n",
    "model = keras.models.load_model(\"tfidf_2gram.keras\")\n",
    "print(f\"Test acc: {model.evaluate(tfidf_2gram_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(1,), dtype=\"string\")\n",
    "processed_inputs = text_vectorization(inputs)\n",
    "outputs = model(processed_inputs)\n",
    "inference_model = keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91.17 percent positive\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "raw_text_data = tf.convert_to_tensor([\n",
    "    [\"That was an excellent movie, I loved it.\"],\n",
    "])\n",
    "predictions = inference_model(raw_text_data)\n",
    "print(f\"{float(predictions[0] * 100):.2f} percent positive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "chapter11_part01_introduction.i",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
