{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "11-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a companion notebook for the book [Deep Learning with Python, Second Edition](https://www.manning.com/books/deep-learning-with-python-second-edition?a_aid=keras&a_bid=76564dff). For readability, it only contains runnable code blocks and section titles, and omits everything else in the book: text paragraphs, figures, and pseudocode.\n",
    "\n",
    "**If you want to be able to follow what's going on, I recommend reading the notebook side by side with your copy of the book.**\n",
    "\n",
    "This notebook was generated for TensorFlow 2.6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## The Transformer architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Understanding self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "#### Generalized self-attention: the query-key-value model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Multi-head attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### The Transformer encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Getting the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0 80.2M    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
      "  0 80.2M    0  176k    0     0  83896      0  0:16:42  0:00:02  0:16:40 83903\n",
      "  0 80.2M    0  560k    0     0   176k      0  0:07:44  0:00:03  0:07:41  176k\n",
      "  1 80.2M    1  944k    0     0   225k      0  0:06:04  0:00:04  0:06:00  225k\n",
      "  1 80.2M    1 1328k    0     0   254k      0  0:05:22  0:00:05  0:05:17  261k\n",
      "  2 80.2M    2 1712k    0     0   274k      0  0:04:59  0:00:06  0:04:53  334k\n",
      "  2 80.2M    2 2096k    0     0   288k      0  0:04:44  0:00:07  0:04:37  375k\n",
      "  2 80.2M    2 2416k    0     0   297k      0  0:04:35  0:00:08  0:04:27  375k\n",
      "  3 80.2M    3 2800k    0     0   306k      0  0:04:27  0:00:09  0:04:18  375k\n",
      "  3 80.2M    3 3184k    0     0   313k      0  0:04:21  0:00:10  0:04:11  375k\n",
      "  4 80.2M    4 3568k    0     0   319k      0  0:04:17  0:00:11  0:04:06  375k\n",
      "  4 80.2M    4 3936k    0     0   322k      0  0:04:14  0:00:12  0:04:02  372k\n",
      "  5 80.2M    5 4320k    0     0   326k      0  0:04:11  0:00:13  0:03:58  372k\n",
      "  5 80.2M    5 4704k    0     0   330k      0  0:04:08  0:00:14  0:03:54  372k\n",
      "  6 80.2M    6 5056k    0     0   334k      0  0:04:05  0:00:15  0:03:50  378k\n",
      "  6 80.2M    6 5408k    0     0   335k      0  0:04:04  0:00:16  0:03:48  372k\n",
      "  6 80.2M    6 5664k    0     0   330k      0  0:04:08  0:00:17  0:03:51  349k\n",
      "  7 80.2M    7 6032k    0     0   332k      0  0:04:07  0:00:18  0:03:49  346k\n",
      "  7 80.2M    7 6400k    0     0   333k      0  0:04:06  0:00:19  0:03:47  343k\n",
      "  8 80.2M    8 6784k    0     0   335k      0  0:04:04  0:00:20  0:03:44  338k\n",
      "  8 80.2M    8 7168k    0     0   337k      0  0:04:03  0:00:21  0:03:42  344k\n",
      "  9 80.2M    9 7552k    0     0   339k      0  0:04:02  0:00:22  0:03:40  369k\n",
      "  9 80.2M    9 7872k    0     0   340k      0  0:04:01  0:00:23  0:03:38  371k\n",
      " 10 80.2M   10 8224k    0     0   340k      0  0:04:01  0:00:24  0:03:37  367k\n",
      " 10 80.2M   10 8608k    0     0   342k      0  0:04:00  0:00:25  0:03:35  368k\n",
      " 10 80.2M   10 8992k    0     0   343k      0  0:03:59  0:00:26  0:03:33  368k\n",
      " 11 80.2M   11 9376k    0     0   344k      0  0:03:58  0:00:27  0:03:31  368k\n",
      " 11 80.2M   11 9760k    0     0   345k      0  0:03:57  0:00:28  0:03:29  368k\n",
      " 12 80.2M   12  9.9M    0     0   346k      0  0:03:56  0:00:29  0:03:27  376k\n",
      " 12 80.2M   12 10.2M    0     0   347k      0  0:03:56  0:00:30  0:03:26  375k\n",
      " 13 80.2M   13 10.5M    0     0   348k      0  0:03:55  0:00:31  0:03:24  375k\n",
      " 13 80.2M   13 10.9M    0     0   349k      0  0:03:55  0:00:32  0:03:23  375k\n",
      " 14 80.2M   14 11.3M    0     0   350k      0  0:03:54  0:00:33  0:03:21  375k\n",
      " 14 80.2M   14 11.7M    0     0   350k      0  0:03:54  0:00:34  0:03:20  372k\n",
      " 15 80.2M   15 12.0M    0     0   351k      0  0:03:53  0:00:35  0:03:18  373k\n",
      " 15 80.2M   15 12.4M    0     0   352k      0  0:03:53  0:00:36  0:03:17  372k\n",
      " 15 80.2M   15 12.8M    0     0   352k      0  0:03:52  0:00:37  0:03:15  372k\n",
      " 16 80.2M   16 13.1M    0     0   353k      0  0:03:52  0:00:38  0:03:14  372k\n",
      " 16 80.2M   16 13.5M    0     0   353k      0  0:03:52  0:00:39  0:03:13  375k\n",
      " 17 80.2M   17 13.8M    0     0   354k      0  0:03:51  0:00:40  0:03:11  375k\n",
      " 17 80.2M   17 14.2M    0     0   354k      0  0:03:51  0:00:41  0:03:10  375k\n",
      " 18 80.2M   18 14.6M    0     0   355k      0  0:03:51  0:00:42  0:03:09  376k\n",
      " 18 80.2M   18 14.9M    0     0   353k      0  0:03:52  0:00:43  0:03:09  357k\n",
      " 18 80.2M   18 15.2M    0     0   352k      0  0:03:52  0:00:44  0:03:08  344k\n",
      " 19 80.2M   19 15.5M    0     0   353k      0  0:03:52  0:00:45  0:03:07  342k\n",
      " 19 80.2M   19 15.8M    0     0   351k      0  0:03:53  0:00:46  0:03:07  322k\n",
      " 20 80.2M   20 16.1M    0     0   350k      0  0:03:54  0:00:47  0:03:07  312k\n",
      " 20 80.2M   20 16.5M    0     0   350k      0  0:03:54  0:00:48  0:03:06  326k\n",
      " 20 80.2M   20 16.8M    0     0   350k      0  0:03:54  0:00:49  0:03:05  331k\n",
      " 21 80.2M   21 17.2M    0     0   350k      0  0:03:54  0:00:50  0:03:04  331k\n",
      " 21 80.2M   21 17.5M    0     0   351k      0  0:03:53  0:00:51  0:03:02  350k\n",
      " 22 80.2M   22 17.9M    0     0   351k      0  0:03:53  0:00:52  0:03:01  359k\n",
      " 22 80.2M   22 18.2M    0     0   351k      0  0:03:53  0:00:53  0:03:00  362k\n",
      " 23 80.2M   23 18.5M    0     0   351k      0  0:03:53  0:00:54  0:02:59  356k\n",
      " 23 80.2M   23 18.8M    0     0   349k      0  0:03:54  0:00:55  0:02:59  340k\n",
      " 23 80.2M   23 19.1M    0     0   348k      0  0:03:55  0:00:56  0:02:59  320k\n",
      " 24 80.2M   24 19.3M    0     0   347k      0  0:03:56  0:00:57  0:02:59  297k\n",
      " 24 80.2M   24 19.6M    0     0   346k      0  0:03:57  0:00:58  0:02:59  287k\n",
      " 24 80.2M   24 20.0M    0     0   345k      0  0:03:57  0:00:59  0:02:58  287k\n",
      " 25 80.2M   25 20.2M    0     0   345k      0  0:03:57  0:01:00  0:02:57  294k\n",
      " 25 80.2M   25 20.5M    0     0   344k      0  0:03:58  0:01:01  0:02:57  300k\n",
      " 26 80.2M   26 20.9M    0     0   344k      0  0:03:58  0:01:02  0:02:56  317k\n",
      " 26 80.2M   26 21.2M    0     0   345k      0  0:03:57  0:01:03  0:02:54  333k\n",
      " 27 80.2M   27 21.6M    0     0   345k      0  0:03:57  0:01:04  0:02:53  346k\n",
      " 27 80.2M   27 22.0M    0     0   346k      0  0:03:57  0:01:05  0:02:52  357k\n",
      " 27 80.2M   27 22.4M    0     0   346k      0  0:03:56  0:01:06  0:02:50  369k\n",
      " 28 80.2M   28 22.7M    0     0   347k      0  0:03:56  0:01:07  0:02:49  376k\n",
      " 28 80.2M   28 23.1M    0     0   347k      0  0:03:56  0:01:08  0:02:48  376k\n",
      " 29 80.2M   29 23.4M    0     0   347k      0  0:03:56  0:01:09  0:02:47  376k\n",
      " 29 80.2M   29 23.7M    0     0   346k      0  0:03:56  0:01:10  0:02:46  355k\n",
      " 30 80.2M   30 24.0M    0     0   346k      0  0:03:57  0:01:11  0:02:46  345k\n",
      " 30 80.2M   30 24.4M    0     0   346k      0  0:03:57  0:01:12  0:02:45  338k\n",
      " 30 80.2M   30 24.7M    0     0   346k      0  0:03:57  0:01:13  0:02:44  330k\n",
      " 31 80.2M   31 25.1M    0     0   346k      0  0:03:56  0:01:14  0:02:42  333k\n",
      " 31 80.2M   31 25.4M    0     0   346k      0  0:03:57  0:01:15  0:02:42  343k\n",
      " 32 80.2M   32 25.7M    0     0   347k      0  0:03:56  0:01:16  0:02:40  353k\n",
      " 32 80.2M   32 26.1M    0     0   347k      0  0:03:56  0:01:17  0:02:39  360k\n",
      " 33 80.2M   33 26.5M    0     0   347k      0  0:03:56  0:01:18  0:02:38  369k\n",
      " 33 80.2M   33 26.9M    0     0   348k      0  0:03:55  0:01:19  0:02:36  365k\n",
      " 34 80.2M   34 27.2M    0     0   348k      0  0:03:55  0:01:20  0:02:35  375k\n",
      " 34 80.2M   34 27.6M    0     0   348k      0  0:03:55  0:01:21  0:02:34  375k\n",
      " 34 80.2M   34 28.0M    0     0   349k      0  0:03:55  0:01:22  0:02:33  382k\n",
      " 35 80.2M   35 28.3M    0     0   349k      0  0:03:55  0:01:23  0:02:32  375k\n",
      " 35 80.2M   35 28.7M    0     0   349k      0  0:03:54  0:01:24  0:02:30  376k\n",
      " 36 80.2M   36 29.1M    0     0   350k      0  0:03:54  0:01:25  0:02:29  375k\n",
      " 36 80.2M   36 29.4M    0     0   350k      0  0:03:54  0:01:26  0:02:28  375k\n",
      " 37 80.2M   37 29.8M    0     0   350k      0  0:03:54  0:01:27  0:02:27  368k\n",
      " 37 80.2M   37 30.2M    0     0   350k      0  0:03:54  0:01:28  0:02:26  375k\n",
      " 38 80.2M   38 30.5M    0     0   351k      0  0:03:53  0:01:29  0:02:24  382k\n",
      " 38 80.2M   38 30.9M    0     0   351k      0  0:03:53  0:01:30  0:02:23  381k\n",
      " 38 80.2M   38 31.2M    0     0   351k      0  0:03:53  0:01:31  0:02:22  372k\n",
      " 39 80.2M   39 31.6M    0     0   351k      0  0:03:53  0:01:32  0:02:21  372k\n",
      " 39 80.2M   39 32.0M    0     0   352k      0  0:03:53  0:01:33  0:02:20  372k\n",
      " 40 80.2M   40 32.4M    0     0   352k      0  0:03:53  0:01:34  0:02:19  365k\n",
      " 40 80.2M   40 32.6M    0     0   351k      0  0:03:53  0:01:35  0:02:18  341k\n",
      " 41 80.2M   41 33.0M    0     0   351k      0  0:03:53  0:01:36  0:02:17  344k\n",
      " 41 80.2M   41 33.3M    0     0   351k      0  0:03:53  0:01:37  0:02:16  344k\n",
      " 42 80.2M   42 33.7M    0     0   351k      0  0:03:53  0:01:38  0:02:15  345k\n",
      " 42 80.2M   42 34.0M    0     0   351k      0  0:03:53  0:01:39  0:02:14  343k\n",
      " 42 80.2M   42 34.4M    0     0   352k      0  0:03:53  0:01:40  0:02:13  369k\n",
      " 43 80.2M   43 34.8M    0     0   352k      0  0:03:53  0:01:41  0:02:12  376k\n",
      " 43 80.2M   43 35.1M    0     0   352k      0  0:03:52  0:01:42  0:02:10  375k\n",
      " 44 80.2M   44 35.5M    0     0   352k      0  0:03:52  0:01:43  0:02:09  373k\n",
      " 44 80.2M   44 35.9M    0     0   353k      0  0:03:52  0:01:44  0:02:08  376k\n",
      " 45 80.2M   45 36.3M    0     0   353k      0  0:03:52  0:01:45  0:02:07  376k\n",
      " 45 80.2M   45 36.6M    0     0   353k      0  0:03:52  0:01:46  0:02:06  378k\n",
      " 46 80.2M   46 37.0M    0     0   353k      0  0:03:52  0:01:47  0:02:05  376k\n",
      " 46 80.2M   46 37.3M    0     0   353k      0  0:03:52  0:01:48  0:02:04  376k\n",
      " 47 80.2M   47 37.7M    0     0   354k      0  0:03:51  0:01:49  0:02:02  376k\n",
      " 47 80.2M   47 38.1M    0     0   354k      0  0:03:51  0:01:50  0:02:01  375k\n",
      " 47 80.2M   47 38.4M    0     0   353k      0  0:03:52  0:01:51  0:02:01  360k\n",
      " 48 80.2M   48 38.8M    0     0   354k      0  0:03:51  0:01:52  0:01:59  363k\n",
      " 48 80.2M   48 39.1M    0     0   354k      0  0:03:51  0:01:53  0:01:58  360k\n",
      " 49 80.2M   49 39.5M    0     0   354k      0  0:03:51  0:01:54  0:01:57  362k\n",
      " 49 80.2M   49 39.8M    0     0   354k      0  0:03:51  0:01:55  0:01:56  359k\n",
      " 50 80.2M   50 40.2M    0     0   354k      0  0:03:51  0:01:56  0:01:55  372k\n",
      " 50 80.2M   50 40.6M    0     0   354k      0  0:03:51  0:01:57  0:01:54  372k\n",
      " 51 80.2M   51 40.9M    0     0   355k      0  0:03:51  0:01:58  0:01:53  376k\n",
      " 51 80.2M   51 41.3M    0     0   355k      0  0:03:51  0:01:59  0:01:52  373k\n",
      " 52 80.2M   52 41.7M    0     0   355k      0  0:03:51  0:02:00  0:01:51  376k\n",
      " 52 80.2M   52 42.1M    0     0   355k      0  0:03:50  0:02:01  0:01:49  376k\n",
      " 52 80.2M   52 42.4M    0     0   356k      0  0:03:50  0:02:02  0:01:48  382k\n",
      " 53 80.2M   53 42.7M    0     0   355k      0  0:03:50  0:02:03  0:01:47  376k\n",
      " 53 80.2M   53 43.1M    0     0   356k      0  0:03:50  0:02:04  0:01:46  376k\n",
      " 54 80.2M   54 43.5M    0     0   356k      0  0:03:50  0:02:05  0:01:45  376k\n",
      " 54 80.2M   54 43.9M    0     0   356k      0  0:03:50  0:02:06  0:01:44  376k\n",
      " 55 80.2M   55 44.2M    0     0   356k      0  0:03:50  0:02:07  0:01:43  370k\n",
      " 55 80.2M   55 44.6M    0     0   356k      0  0:03:50  0:02:08  0:01:42  373k\n",
      " 56 80.2M   56 44.9M    0     0   356k      0  0:03:50  0:02:09  0:01:41  359k\n",
      " 56 80.2M   56 45.2M    0     0   355k      0  0:03:50  0:02:10  0:01:40  345k\n",
      " 56 80.2M   56 45.5M    0     0   355k      0  0:03:50  0:02:11  0:01:39  339k\n",
      " 57 80.2M   57 45.9M    0     0   355k      0  0:03:50  0:02:12  0:01:38  333k\n",
      " 57 80.2M   57 46.2M    0     0   355k      0  0:03:50  0:02:13  0:01:37  329k\n",
      " 58 80.2M   58 46.5M    0     0   355k      0  0:03:51  0:02:14  0:01:37  337k\n",
      " 58 80.2M   58 46.9M    0     0   355k      0  0:03:50  0:02:15  0:01:35  351k\n",
      " 58 80.2M   58 47.3M    0     0   356k      0  0:03:50  0:02:16  0:01:34  362k\n",
      " 59 80.2M   59 47.7M    0     0   356k      0  0:03:50  0:02:17  0:01:33  368k\n",
      " 59 80.2M   59 48.0M    0     0   356k      0  0:03:50  0:02:18  0:01:32  369k\n",
      " 60 80.2M   60 48.3M    0     0   355k      0  0:03:50  0:02:19  0:01:31  359k\n",
      " 60 80.2M   60 48.6M    0     0   355k      0  0:03:51  0:02:20  0:01:31  349k\n",
      " 61 80.2M   61 49.0M    0     0   355k      0  0:03:51  0:02:21  0:01:30  341k\n",
      " 61 80.2M   61 49.3M    0     0   355k      0  0:03:50  0:02:22  0:01:28  341k\n",
      " 61 80.2M   61 49.6M    0     0   355k      0  0:03:51  0:02:23  0:01:28  332k\n",
      " 62 80.2M   62 49.9M    0     0   354k      0  0:03:51  0:02:24  0:01:27  332k\n",
      " 62 80.2M   62 50.2M    0     0   354k      0  0:03:51  0:02:25  0:01:26  329k\n",
      " 63 80.2M   63 50.5M    0     0   354k      0  0:03:51  0:02:26  0:01:25  320k\n",
      " 63 80.2M   63 50.8M    0     0   353k      0  0:03:52  0:02:27  0:01:25  294k\n",
      " 63 80.2M   63 51.0M    0     0   353k      0  0:03:52  0:02:28  0:01:24  291k\n",
      " 64 80.2M   64 51.4M    0     0   352k      0  0:03:52  0:02:29  0:01:23  294k\n",
      " 64 80.2M   64 51.7M    0     0   352k      0  0:03:52  0:02:30  0:01:22  294k\n",
      " 64 80.2M   64 52.0M    0     0   352k      0  0:03:53  0:02:31  0:01:22  291k\n",
      " 65 80.2M   65 52.3M    0     0   352k      0  0:03:53  0:02:32  0:01:21  310k\n",
      " 65 80.2M   65 52.7M    0     0   352k      0  0:03:53  0:02:33  0:01:20  328k\n",
      " 66 80.2M   66 53.0M    0     0   352k      0  0:03:53  0:02:34  0:01:19  340k\n",
      " 66 80.2M   66 53.3M    0     0   351k      0  0:03:53  0:02:35  0:01:18  330k\n",
      " 66 80.2M   66 53.6M    0     0   351k      0  0:03:53  0:02:36  0:01:17  336k\n",
      " 67 80.2M   67 54.0M    0     0   351k      0  0:03:53  0:02:37  0:01:16  343k\n",
      " 67 80.2M   67 54.3M    0     0   352k      0  0:03:53  0:02:38  0:01:15  343k\n",
      " 68 80.2M   68 54.7M    0     0   352k      0  0:03:53  0:02:39  0:01:14  344k\n",
      " 68 80.2M   68 55.1M    0     0   352k      0  0:03:53  0:02:40  0:01:13  366k\n",
      " 69 80.2M   69 55.5M    0     0   352k      0  0:03:53  0:02:41  0:01:12  376k\n",
      " 69 80.2M   69 55.8M    0     0   352k      0  0:03:52  0:02:42  0:01:10  376k\n",
      " 70 80.2M   70 56.2M    0     0   352k      0  0:03:52  0:02:43  0:01:09  376k\n",
      " 70 80.2M   70 56.5M    0     0   352k      0  0:03:52  0:02:44  0:01:08  375k\n",
      " 70 80.2M   70 56.9M    0     0   352k      0  0:03:52  0:02:45  0:01:07  372k\n",
      " 71 80.2M   71 57.3M    0     0   353k      0  0:03:52  0:02:46  0:01:06  372k\n",
      " 71 80.2M   71 57.6M    0     0   353k      0  0:03:52  0:02:47  0:01:05  373k\n",
      " 72 80.2M   72 58.0M    0     0   353k      0  0:03:52  0:02:48  0:01:04  373k\n",
      " 72 80.2M   72 58.4M    0     0   353k      0  0:03:52  0:02:49  0:01:03  373k\n",
      " 73 80.2M   73 58.7M    0     0   353k      0  0:03:52  0:02:50  0:01:02  376k\n",
      " 73 80.2M   73 59.1M    0     0   353k      0  0:03:52  0:02:51  0:01:01  376k\n",
      " 74 80.2M   74 59.5M    0     0   353k      0  0:03:52  0:02:52  0:01:00  376k\n",
      " 74 80.2M   74 59.8M    0     0   354k      0  0:03:52  0:02:53  0:00:59  376k\n",
      " 75 80.2M   75 60.2M    0     0   354k      0  0:03:51  0:02:54  0:00:57  376k\n",
      " 75 80.2M   75 60.5M    0     0   353k      0  0:03:52  0:02:55  0:00:57  357k\n",
      " 75 80.2M   75 60.8M    0     0   353k      0  0:03:52  0:02:56  0:00:56  348k\n",
      " 76 80.2M   76 61.2M    0     0   353k      0  0:03:52  0:02:57  0:00:55  336k\n",
      " 76 80.2M   76 61.3M    0     0   352k      0  0:03:52  0:02:58  0:00:54  314k\n",
      " 76 80.2M   76 61.5M    0     0   352k      0  0:03:53  0:02:59  0:00:54  278k\n",
      " 77 80.2M   77 61.8M    0     0   351k      0  0:03:53  0:03:00  0:00:53  265k\n",
      " 77 80.2M   77 62.0M    0     0   350k      0  0:03:54  0:03:01  0:00:53  243k\n",
      " 77 80.2M   77 62.2M    0     0   349k      0  0:03:54  0:03:02  0:00:52  221k\n",
      " 77 80.2M   77 62.5M    0     0   349k      0  0:03:54  0:03:03  0:00:51  232k\n",
      " 78 80.2M   78 62.9M    0     0   349k      0  0:03:54  0:03:04  0:00:50  263k\n",
      " 78 80.2M   78 63.3M    0     0   349k      0  0:03:54  0:03:05  0:00:49  297k\n",
      " 79 80.2M   79 63.4M    0     0   349k      0  0:03:55  0:03:06  0:00:49  301k\n",
      " 79 80.2M   79 63.8M    0     0   349k      0  0:03:55  0:03:07  0:00:48  320k\n",
      " 79 80.2M   79 64.1M    0     0   349k      0  0:03:55  0:03:08  0:00:47  336k\n",
      " 80 80.2M   80 64.5M    0     0   349k      0  0:03:55  0:03:09  0:00:46  340k\n",
      " 80 80.2M   80 64.9M    0     0   349k      0  0:03:55  0:03:10  0:00:45  339k\n",
      " 81 80.2M   81 65.2M    0     0   349k      0  0:03:54  0:03:11  0:00:43  363k\n",
      " 81 80.2M   81 65.7M    0     0   349k      0  0:03:54  0:03:12  0:00:42  378k\n",
      " 82 80.2M   82 66.0M    0     0   349k      0  0:03:54  0:03:13  0:00:41  366k\n",
      " 82 80.2M   82 66.3M    0     0   349k      0  0:03:54  0:03:14  0:00:40  365k\n",
      " 83 80.2M   83 66.6M    0     0   349k      0  0:03:54  0:03:15  0:00:39  365k\n",
      " 83 80.2M   83 67.0M    0     0   350k      0  0:03:54  0:03:16  0:00:38  365k\n",
      " 84 80.2M   84 67.4M    0     0   350k      0  0:03:54  0:03:17  0:00:37  359k\n",
      " 84 80.2M   84 67.8M    0     0   350k      0  0:03:54  0:03:18  0:00:36  376k\n",
      " 84 80.2M   84 68.1M    0     0   350k      0  0:03:54  0:03:19  0:00:35  376k\n",
      " 85 80.2M   85 68.5M    0     0   350k      0  0:03:54  0:03:20  0:00:34  376k\n",
      " 85 80.2M   85 68.9M    0     0   350k      0  0:03:54  0:03:21  0:00:33  376k\n",
      " 86 80.2M   86 69.2M    0     0   350k      0  0:03:54  0:03:22  0:00:32  376k\n",
      " 86 80.2M   86 69.6M    0     0   351k      0  0:03:54  0:03:23  0:00:31  376k\n",
      " 87 80.2M   87 70.0M    0     0   351k      0  0:03:53  0:03:24  0:00:29  376k\n",
      " 87 80.2M   87 70.3M    0     0   351k      0  0:03:53  0:03:25  0:00:28  376k\n",
      " 88 80.2M   88 70.7M    0     0   351k      0  0:03:53  0:03:26  0:00:27  376k\n",
      " 88 80.2M   88 71.1M    0     0   351k      0  0:03:53  0:03:27  0:00:26  373k\n",
      " 89 80.2M   89 71.4M    0     0   351k      0  0:03:53  0:03:28  0:00:25  373k\n",
      " 89 80.2M   89 71.8M    0     0   351k      0  0:03:53  0:03:29  0:00:24  373k\n",
      " 89 80.2M   89 72.1M    0     0   351k      0  0:03:53  0:03:30  0:00:23  375k\n",
      " 90 80.2M   90 72.5M    0     0   351k      0  0:03:53  0:03:31  0:00:22  372k\n",
      " 90 80.2M   90 72.9M    0     0   352k      0  0:03:53  0:03:32  0:00:21  376k\n",
      " 91 80.2M   91 73.2M    0     0   352k      0  0:03:53  0:03:33  0:00:20  376k\n",
      " 91 80.2M   91 73.6M    0     0   352k      0  0:03:53  0:03:34  0:00:19  376k\n",
      " 92 80.2M   92 74.0M    0     0   352k      0  0:03:53  0:03:35  0:00:18  374k\n",
      " 92 80.2M   92 74.4M    0     0   352k      0  0:03:53  0:03:36  0:00:17  376k\n",
      " 93 80.2M   93 74.7M    0     0   352k      0  0:03:53  0:03:37  0:00:16  376k\n",
      " 93 80.2M   93 75.1M    0     0   352k      0  0:03:52  0:03:38  0:00:14  378k\n",
      " 94 80.2M   94 75.4M    0     0   352k      0  0:03:53  0:03:39  0:00:14  362k\n",
      " 94 80.2M   94 75.7M    0     0   352k      0  0:03:53  0:03:40  0:00:13  346k\n",
      " 94 80.2M   94 76.0M    0     0   351k      0  0:03:53  0:03:41  0:00:12  329k\n",
      " 95 80.2M   95 76.3M    0     0   351k      0  0:03:53  0:03:42  0:00:11  316k\n",
      " 95 80.2M   95 76.6M    0     0   351k      0  0:03:53  0:03:43  0:00:10  306k\n",
      " 95 80.2M   95 77.0M    0     0   351k      0  0:03:53  0:03:44  0:00:09  315k\n",
      " 96 80.2M   96 77.3M    0     0   351k      0  0:03:53  0:03:45  0:00:08  329k\n",
      " 96 80.2M   96 77.6M    0     0   351k      0  0:03:53  0:03:46  0:00:07  345k\n",
      " 97 80.2M   97 78.0M    0     0   351k      0  0:03:53  0:03:47  0:00:06  357k\n",
      " 97 80.2M   97 78.4M    0     0   352k      0  0:03:53  0:03:48  0:00:05  367k\n",
      " 98 80.2M   98 78.8M    0     0   352k      0  0:03:53  0:03:49  0:00:04  374k\n",
      " 98 80.2M   98 79.1M    0     0   352k      0  0:03:53  0:03:50  0:00:03  371k\n",
      " 99 80.2M   99 79.5M    0     0   352k      0  0:03:53  0:03:51  0:00:02  379k\n",
      " 99 80.2M   99 79.8M    0     0   352k      0  0:03:53  0:03:52  0:00:01  375k\n",
      "100 80.2M  100 80.2M    0     0   352k      0  0:03:52  0:03:52 --:--:--  382k\n"
     ]
    }
   ],
   "source": [
    "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "!tar -xf aclImdb_v1.tar.gz\n",
    "#!rm -r aclImdb/train/unsup       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Preparing the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16000 files belonging to 2 classes.\n",
      "Found 4000 files belonging to 2 classes.\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "import os, pathlib, shutil, random\n",
    "from tensorflow import keras\n",
    "batch_size = 32\n",
    "base_dir = pathlib.Path(\"aclImdb\")\n",
    "val_dir = base_dir / \"val\"\n",
    "train_dir = base_dir / \"train\"\n",
    "for category in (\"neg\", \"pos\"):\n",
    "    os.makedirs(val_dir / category)\n",
    "    files = os.listdir(train_dir / category)\n",
    "    random.Random(1337).shuffle(files)\n",
    "    num_val_samples = int(0.2 * len(files))\n",
    "    val_files = files[-num_val_samples:]\n",
    "    for fname in val_files:\n",
    "        shutil.move(train_dir / category / fname,\n",
    "                    val_dir / category / fname)\n",
    "\n",
    "train_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/train\", batch_size=batch_size\n",
    ")\n",
    "val_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/val\", batch_size=batch_size\n",
    ")\n",
    "test_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/test\", batch_size=batch_size\n",
    ")\n",
    "text_only_train_ds = train_ds.map(lambda x, y: x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Vectorizing the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "max_length = 600\n",
    "max_tokens = 20000\n",
    "text_vectorization = layers.TextVectorization(\n",
    "    max_tokens=max_tokens,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=max_length,\n",
    ")\n",
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "int_train_ds = train_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "int_val_ds = val_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "int_test_ds = test_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Transformer encoder implemented as a subclassed `Layer`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
    "             layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        if mask is not None:\n",
    "            mask = mask[:, tf.newaxis, :]\n",
    "        attention_output = self.attention(\n",
    "            inputs, inputs, attention_mask=mask)\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"dense_dim\": self.dense_dim,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Using the Transformer encoder for text classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, None, 256)         5120000   \n",
      "_________________________________________________________________\n",
      "transformer_encoder (Transfo (None, None, 256)         543776    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 5,664,033\n",
      "Trainable params: 5,664,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 20000\n",
    "embed_dim = 256\n",
    "num_heads = 2\n",
    "dense_dim = 32\n",
    "\n",
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "x = layers.Embedding(vocab_size, embed_dim)(inputs)\n",
    "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Training and evaluating the Transformer encoder based model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "500/500 [==============================] - 24s 41ms/step - loss: 0.5170 - accuracy: 0.7539 - val_loss: 0.4039 - val_accuracy: 0.8155\n",
      "Epoch 2/20\n",
      "500/500 [==============================] - 20s 40ms/step - loss: 0.3285 - accuracy: 0.8566 - val_loss: 0.3426 - val_accuracy: 0.8545\n",
      "Epoch 3/20\n",
      "500/500 [==============================] - 20s 40ms/step - loss: 0.2592 - accuracy: 0.8946 - val_loss: 0.2868 - val_accuracy: 0.8808\n",
      "Epoch 4/20\n",
      "500/500 [==============================] - 20s 40ms/step - loss: 0.1901 - accuracy: 0.9248 - val_loss: 0.3051 - val_accuracy: 0.8898\n",
      "Epoch 5/20\n",
      "500/500 [==============================] - 20s 40ms/step - loss: 0.1443 - accuracy: 0.9446 - val_loss: 0.3559 - val_accuracy: 0.8923\n",
      "Epoch 6/20\n",
      "500/500 [==============================] - 20s 40ms/step - loss: 0.1171 - accuracy: 0.9563 - val_loss: 0.4114 - val_accuracy: 0.8702\n",
      "Epoch 7/20\n",
      "500/500 [==============================] - 20s 40ms/step - loss: 0.0997 - accuracy: 0.9636 - val_loss: 0.4378 - val_accuracy: 0.8767\n",
      "Epoch 8/20\n",
      "500/500 [==============================] - 20s 40ms/step - loss: 0.0814 - accuracy: 0.9718 - val_loss: 0.4284 - val_accuracy: 0.8727\n",
      "Epoch 9/20\n",
      "500/500 [==============================] - 20s 40ms/step - loss: 0.0682 - accuracy: 0.9764 - val_loss: 0.5882 - val_accuracy: 0.8575\n",
      "Epoch 10/20\n",
      "500/500 [==============================] - 20s 40ms/step - loss: 0.0580 - accuracy: 0.9789 - val_loss: 0.6136 - val_accuracy: 0.8595\n",
      "Epoch 11/20\n",
      "500/500 [==============================] - 20s 40ms/step - loss: 0.0475 - accuracy: 0.9827 - val_loss: 0.7136 - val_accuracy: 0.8558\n",
      "Epoch 12/20\n",
      "500/500 [==============================] - 20s 40ms/step - loss: 0.0437 - accuracy: 0.9854 - val_loss: 0.7239 - val_accuracy: 0.8650\n",
      "Epoch 13/20\n",
      "500/500 [==============================] - 20s 40ms/step - loss: 0.0400 - accuracy: 0.9861 - val_loss: 0.7632 - val_accuracy: 0.8533\n",
      "Epoch 14/20\n",
      "500/500 [==============================] - 20s 40ms/step - loss: 0.0308 - accuracy: 0.9895 - val_loss: 0.7868 - val_accuracy: 0.8627\n",
      "Epoch 15/20\n",
      "500/500 [==============================] - 20s 40ms/step - loss: 0.0304 - accuracy: 0.9908 - val_loss: 0.7987 - val_accuracy: 0.8515\n",
      "Epoch 16/20\n",
      "500/500 [==============================] - 20s 40ms/step - loss: 0.0295 - accuracy: 0.9901 - val_loss: 0.8155 - val_accuracy: 0.8550\n",
      "Epoch 17/20\n",
      "500/500 [==============================] - 20s 40ms/step - loss: 0.0230 - accuracy: 0.9928 - val_loss: 0.9817 - val_accuracy: 0.8660\n",
      "Epoch 18/20\n",
      "500/500 [==============================] - 20s 40ms/step - loss: 0.0166 - accuracy: 0.9951 - val_loss: 0.8429 - val_accuracy: 0.8733\n",
      "Epoch 19/20\n",
      "500/500 [==============================] - 20s 40ms/step - loss: 0.0163 - accuracy: 0.9950 - val_loss: 0.9804 - val_accuracy: 0.8535\n",
      "Epoch 20/20\n",
      "500/500 [==============================] - 20s 41ms/step - loss: 0.0134 - accuracy: 0.9956 - val_loss: 0.9825 - val_accuracy: 0.8620\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.3120 - accuracy: 0.8656\n",
      "Test acc: 0.866\n"
     ]
    }
   ],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"transformer_encoder.keras\",\n",
    "                                    save_best_only=True)\n",
    "]\n",
    "model.fit(int_train_ds, validation_data=int_val_ds, epochs=20, callbacks=callbacks)\n",
    "model = keras.models.load_model(\n",
    "    \"transformer_encoder.keras\",\n",
    "    custom_objects={\"TransformerEncoder\": TransformerEncoder})\n",
    "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "#### Using positional encoding to re-inject order information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Implementing positional embedding as a subclassed layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.token_embeddings = layers.Embedding(\n",
    "            input_dim=input_dim, output_dim=output_dim)\n",
    "        self.position_embeddings = layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=output_dim)\n",
    "        self.sequence_length = sequence_length\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return tf.math.not_equal(inputs, 0)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"output_dim\": self.output_dim,\n",
    "            \"sequence_length\": self.sequence_length,\n",
    "            \"input_dim\": self.input_dim,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "#### Putting it all together: A text-classification Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Combining the Transformer encoder with positional embedding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "positional_embedding (Positi (None, None, 256)         5273600   \n",
      "_________________________________________________________________\n",
      "transformer_encoder_1 (Trans (None, None, 256)         543776    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 5,817,633\n",
      "Trainable params: 5,817,633\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "500/500 [==============================] - 22s 42ms/step - loss: 0.5181 - accuracy: 0.7546 - val_loss: 0.3242 - val_accuracy: 0.8708\n",
      "Epoch 2/20\n",
      "500/500 [==============================] - 21s 42ms/step - loss: 0.2456 - accuracy: 0.9074 - val_loss: 0.3484 - val_accuracy: 0.8625\n",
      "Epoch 3/20\n",
      "500/500 [==============================] - 21s 42ms/step - loss: 0.1759 - accuracy: 0.9337 - val_loss: 0.2860 - val_accuracy: 0.8938\n",
      "Epoch 4/20\n",
      "500/500 [==============================] - 21s 42ms/step - loss: 0.1390 - accuracy: 0.9498 - val_loss: 0.2976 - val_accuracy: 0.8845\n",
      "Epoch 5/20\n",
      "500/500 [==============================] - 21s 42ms/step - loss: 0.1105 - accuracy: 0.9593 - val_loss: 0.3767 - val_accuracy: 0.8870\n",
      "Epoch 6/20\n",
      "500/500 [==============================] - 21s 42ms/step - loss: 0.0958 - accuracy: 0.9671 - val_loss: 0.4439 - val_accuracy: 0.8842\n",
      "Epoch 7/20\n",
      "500/500 [==============================] - 21s 42ms/step - loss: 0.0817 - accuracy: 0.9713 - val_loss: 0.4804 - val_accuracy: 0.8715\n",
      "Epoch 8/20\n",
      "500/500 [==============================] - 21s 42ms/step - loss: 0.0683 - accuracy: 0.9764 - val_loss: 0.4202 - val_accuracy: 0.8763\n",
      "Epoch 9/20\n",
      "500/500 [==============================] - 21s 42ms/step - loss: 0.0569 - accuracy: 0.9804 - val_loss: 0.4735 - val_accuracy: 0.8777\n",
      "Epoch 10/20\n",
      "500/500 [==============================] - 21s 42ms/step - loss: 0.0528 - accuracy: 0.9826 - val_loss: 0.5821 - val_accuracy: 0.8648\n",
      "Epoch 11/20\n",
      "500/500 [==============================] - 21s 42ms/step - loss: 0.0438 - accuracy: 0.9856 - val_loss: 0.5251 - val_accuracy: 0.8742\n",
      "Epoch 12/20\n",
      "500/500 [==============================] - 21s 42ms/step - loss: 0.0367 - accuracy: 0.9879 - val_loss: 0.9738 - val_accuracy: 0.8325\n",
      "Epoch 13/20\n",
      "500/500 [==============================] - 21s 42ms/step - loss: 0.0298 - accuracy: 0.9903 - val_loss: 0.5787 - val_accuracy: 0.8677\n",
      "Epoch 14/20\n",
      "500/500 [==============================] - 21s 42ms/step - loss: 0.0267 - accuracy: 0.9911 - val_loss: 0.8293 - val_accuracy: 0.8698\n",
      "Epoch 15/20\n",
      "500/500 [==============================] - 21s 42ms/step - loss: 0.0233 - accuracy: 0.9928 - val_loss: 0.7254 - val_accuracy: 0.8365\n",
      "Epoch 16/20\n",
      "500/500 [==============================] - 21s 42ms/step - loss: 0.0203 - accuracy: 0.9933 - val_loss: 0.7673 - val_accuracy: 0.8695\n",
      "Epoch 17/20\n",
      "500/500 [==============================] - 21s 42ms/step - loss: 0.0188 - accuracy: 0.9937 - val_loss: 0.7184 - val_accuracy: 0.8620\n",
      "Epoch 18/20\n",
      "500/500 [==============================] - 21s 43ms/step - loss: 0.0160 - accuracy: 0.9952 - val_loss: 0.7586 - val_accuracy: 0.8737\n",
      "Epoch 19/20\n",
      "500/500 [==============================] - 21s 42ms/step - loss: 0.0116 - accuracy: 0.9969 - val_loss: 0.8837 - val_accuracy: 0.8760\n",
      "Epoch 20/20\n",
      "500/500 [==============================] - 21s 42ms/step - loss: 0.0080 - accuracy: 0.9971 - val_loss: 1.0774 - val_accuracy: 0.8572\n",
      "782/782 [==============================] - 12s 14ms/step - loss: 0.3275 - accuracy: 0.8740\n",
      "Test acc: 0.874\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 20000\n",
    "sequence_length = 600\n",
    "embed_dim = 256\n",
    "num_heads = 2\n",
    "dense_dim = 32\n",
    "\n",
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)\n",
    "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"full_transformer_encoder.keras\",\n",
    "                                    save_best_only=True)\n",
    "]\n",
    "model.fit(int_train_ds, validation_data=int_val_ds, epochs=20, callbacks=callbacks)\n",
    "model = keras.models.load_model(\n",
    "    \"full_transformer_encoder.keras\",\n",
    "    custom_objects={\"TransformerEncoder\": TransformerEncoder,\n",
    "                    \"PositionalEmbedding\": PositionalEmbedding})\n",
    "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### When to use sequence models over bag-of-words models?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "chapter11_part03_transformer.i",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
